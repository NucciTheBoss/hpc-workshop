{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to NucciTheBoss's HPC workshop! If you are here for the workshop The open-source software behind High-Performance Computing , then you are in the right place! I am excited to give you an introduction to the open-source software that powers HPC on Ubuntu at the 2022 Ubuntu Summit in Prague. Forword While this workshop is set at the intermediate level, this workshop provides a beginner-friendly introduction to the open-source software behind HPC on Ubuntu. The reason this workshop is set to intermediate the level is because I expect you to be very comfortable with working from the terminal on Ubuntu. If you are not familiar with executing complex bash commands, editing files, and/or managing mulitple sessions from the terminal, then you may struggle with this workshop. I will be focusing on teaching folks how to build a pseudo-HPC cluster, not how to copy and paste text from the terminal. However , I was once a beginner too, so I will not be mean and stupid by leaving you in the dust. If you do not know how to do something and we have extra time, please call me over. I expect that you have a decent CPU, at least 4 cores, and at least 8 GB of RAM in your machine. You might be able to get away with lower specs, but things may take longer to \"finish.\" If your machine is struggling to keep up, try pairing with another individual and working together with them. I respect potatos with a CPU, but there are just some things that they are not very good at. Now let us get on with the show!","title":"Introduction"},{"location":"#welcome-to-nuccithebosss-hpc-workshop","text":"If you are here for the workshop The open-source software behind High-Performance Computing , then you are in the right place! I am excited to give you an introduction to the open-source software that powers HPC on Ubuntu at the 2022 Ubuntu Summit in Prague.","title":"Welcome to NucciTheBoss's HPC workshop!"},{"location":"#forword","text":"While this workshop is set at the intermediate level, this workshop provides a beginner-friendly introduction to the open-source software behind HPC on Ubuntu. The reason this workshop is set to intermediate the level is because I expect you to be very comfortable with working from the terminal on Ubuntu. If you are not familiar with executing complex bash commands, editing files, and/or managing mulitple sessions from the terminal, then you may struggle with this workshop. I will be focusing on teaching folks how to build a pseudo-HPC cluster, not how to copy and paste text from the terminal. However , I was once a beginner too, so I will not be mean and stupid by leaving you in the dust. If you do not know how to do something and we have extra time, please call me over. I expect that you have a decent CPU, at least 4 cores, and at least 8 GB of RAM in your machine. You might be able to get away with lower specs, but things may take longer to \"finish.\" If your machine is struggling to keep up, try pairing with another individual and working together with them. I respect potatos with a CPU, but there are just some things that they are not very good at. Now let us get on with the show!","title":"Forword"},{"location":"containers/","text":"TODO","title":"Getting fancy with containers"},{"location":"containers/#todo","text":"","title":"TODO"},{"location":"future/","text":"TODO","title":"Where to go from here?"},{"location":"future/#todo","text":"","title":"TODO"},{"location":"groundwork/","text":"So what will we be doing? We are going to be building an HPC cluster with LXD! Well... not really. We will be building a psuedo-HPC cluster, or as I like to call it, a micro-HPC cluster . I got the idea for the name from another popular project: Microstack . True HPC clusters can fill an entire building; ours will just be a few LXD containers on your laptop. Setting up LXD on your system LXD will serve as the undercloud for our HPC cluster. If you do not already have LXD installed on your system, use the following command to install the LXD snap package: $ sudo snap install lxd With the snap installed, set up LXD on your system with the following configuration options: $ lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: micro-hpc Name of the storage backend to use (lvm, zfs, ceph, btrfs, dir) [default=zfs]: Create a new ZFS pool? (yes/no) [default=yes]: Would you like to use an existing empty block device (e.g. a disk or partition)? (yes/no) [default=no]: Size in GB of the new loop device (1GB minimum) [default=27GB]: 60GB Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: Would you like the LXD server to be available over the network? (yes/no) [default=no]: Would you like stale cached images to be updated automatically? (yes/no) [default=yes]: Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: Getting the hpc-workshop pip package To make things go a \"faster\", I have written a small program to handle setting up what you will need for the micro-HPC cluster to work. It can be installed as pip package from PyPI: $ pip install hpc-workshop After installing the package, you should be able to access the hpc-workshop command. Note: You may have to run the command export PATH=$HOME/.local/bin:$PATH to access the hpc-workshop executable. Bootstrapping your cluster With hpc-workshop installed, all you to do is execute the following command: $ hpc-workshop init Yes, really, it is that simple! Now onto creating our cluster's user.","title":"Groundwork"},{"location":"groundwork/#so-what-will-we-be-doing","text":"We are going to be building an HPC cluster with LXD! Well... not really. We will be building a psuedo-HPC cluster, or as I like to call it, a micro-HPC cluster . I got the idea for the name from another popular project: Microstack . True HPC clusters can fill an entire building; ours will just be a few LXD containers on your laptop.","title":"So what will we be doing?"},{"location":"groundwork/#setting-up-lxd-on-your-system","text":"LXD will serve as the undercloud for our HPC cluster. If you do not already have LXD installed on your system, use the following command to install the LXD snap package: $ sudo snap install lxd With the snap installed, set up LXD on your system with the following configuration options: $ lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: micro-hpc Name of the storage backend to use (lvm, zfs, ceph, btrfs, dir) [default=zfs]: Create a new ZFS pool? (yes/no) [default=yes]: Would you like to use an existing empty block device (e.g. a disk or partition)? (yes/no) [default=no]: Size in GB of the new loop device (1GB minimum) [default=27GB]: 60GB Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: Would you like the LXD server to be available over the network? (yes/no) [default=no]: Would you like stale cached images to be updated automatically? (yes/no) [default=yes]: Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]:","title":"Setting up LXD on your system"},{"location":"groundwork/#getting-the-hpc-workshop-pip-package","text":"To make things go a \"faster\", I have written a small program to handle setting up what you will need for the micro-HPC cluster to work. It can be installed as pip package from PyPI: $ pip install hpc-workshop After installing the package, you should be able to access the hpc-workshop command. Note: You may have to run the command export PATH=$HOME/.local/bin:$PATH to access the hpc-workshop executable.","title":"Getting the hpc-workshop pip package"},{"location":"groundwork/#bootstrapping-your-cluster","text":"With hpc-workshop installed, all you to do is execute the following command: $ hpc-workshop init Yes, really, it is that simple! Now onto creating our cluster's user.","title":"Bootstrapping your cluster"},{"location":"ldap/","text":"Who is going to use our cluster? Obviously, we cannot let root be the sole user of our cluster - that would create a disaster. Therefore, to create a user, and have that user exists across all nodes within our micro-HPC cluster. To accomplish this, we will use OpenLDAP , an open-source implementation of the Lightweight Directory Access Protocol (LDAP). Enabling the LDAP server on ldap-0 Before we can add our user to the cluster, we need to start the OpenLDAP server. Let us start a shell session inside the ldap-0 node: $ lxc shell ldap-0 Now inside the ldap-0 node, execute the following commands to start the OpenLDAP server: ~# systemctl enable slapd ~# systemctl start slapd We are not done yet, however, for we need to also configure the OpenLDAP server. Luckily, dpkg-reconfigure can handle most of the legwork for us: ~# dpkg-reconfigure -f readline slapd You will be taken through an interactive dialog to set up your server. Answer the prompts with the same answers as below: Omit OpenLDAP server configuration? [yes/no] no DNS domain name: micro-hpc.org Organization name: micro-hpc Administrator password: test Confirm password: test Do you want your database to be removed when slapd is purged? [yes/no] yes Move old database? [yes/no] yes Note: For the password prompts, GNU readline will hide your inputs. Do not freak out when you do not see any characters appearing in the terminal when you are creating the adminstrator password. Creating user test and group research on the server Still inside ldap-0 open a text editor window. I used nano in my case: $ nano add_test_user.ldif With the editor open, populate the file with the following LDIF ( LDAP Data Interchange Format ) content, and then save and close the file: dn: ou=People,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: People dn: ou=Groups,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: Groups dn: uid=test,ou=People,dc=micro-hpc,dc=org uid: test objectClass: inetOrgPerson objectClass: posixAccount cn: Test sn: Test givenName: Test mail: test@example.com userPassword: test uidNumber: 10000 gidNumber: 10000 loginShell: /bin/bash homeDirectory: /home/test dn: cn=test,ou=Groups,dc=micro-hpc,dc=org cn: test objectClass: posixGroup gidNumber: 10000 memberUid: nucci dn: cn=research,ou=Groups,dc=micro-hpc,dc=org cn: research objectClass: posixGroup gidNumber: 10100 memberUid: test Important: Make sure the content in your LDIF file is exactly the same as the code block above. Now use the following command to add user test and group research to the OpenLDAP server: ~# ldapadd -x -D \"cn=admin,dc=micro-hpc,dc=org\" -w test -f /root/add_test_user.ldif -H ldap:/// Letting everyone else know about user test Now we need to set up all the other nodes so that they know about user test . To accomplish this, we will use the System Security Services Daemon , also known as SSSD. First, we need to grab the IPv4 address of the ldap-0 node. Execute the following command on your system. Note that you will need to run this command in a terminal window outside ldap-0 : $ lxc list -c n4 -f compact | grep ldap The output from the above command should look similar to the following output: ldap-0 10.5.1.44 (eth0) With the IPv4 address of ldap-0 in hand, open a text editor window: $ nano sssd.conf Now populate the sssd.conf file with the following content: [sssd] config_file_version = 2 domains = micro-hpc.org [domain/micro-hpc.org] id_provider = ldap auth_provider = ldap ldap_uri = ldap://10.5.1.44 cache_credentials = True ldap_search_base = dc=micro-hpc,dc=org Important: You should replace where I have my IPv4 address for ldap-0 with the IPv4 adress of your ldap-0 node. We are almost there! One thing to note with SSSD is that it requires the sssd.conf file to have very specific access permissions. Also, these permissions need to be the same across all nodes connecting to the OpenLDAP server. Let us use some fancy bash scripting to make the set up a little easier on ourselves: $ nodes=( nfs-0 head-0 compute-0 ) $ for i in ${nodes[@]}; do lxc file push sssd.conf $i/etc/sssd/sssd.conf lxc exec $i -- chmod 0600 /etc/sssd/sssd.conf lxc exec $i -- chown root:root /etc/sssd/sssd.conf lxc exec $i -- pam-auth-update --enable mkhomedir lxc exec $i -- systemctl enable sssd lxc exec $i -- systemctl start sssd done This for loop will save us a lot of copy, pasting, and changing a couple characters. Now onto setting up our shared file system!","title":"Setting up our researcher"},{"location":"ldap/#who-is-going-to-use-our-cluster","text":"Obviously, we cannot let root be the sole user of our cluster - that would create a disaster. Therefore, to create a user, and have that user exists across all nodes within our micro-HPC cluster. To accomplish this, we will use OpenLDAP , an open-source implementation of the Lightweight Directory Access Protocol (LDAP).","title":"Who is going to use our cluster?"},{"location":"ldap/#enabling-the-ldap-server-on-ldap-0","text":"Before we can add our user to the cluster, we need to start the OpenLDAP server. Let us start a shell session inside the ldap-0 node: $ lxc shell ldap-0 Now inside the ldap-0 node, execute the following commands to start the OpenLDAP server: ~# systemctl enable slapd ~# systemctl start slapd We are not done yet, however, for we need to also configure the OpenLDAP server. Luckily, dpkg-reconfigure can handle most of the legwork for us: ~# dpkg-reconfigure -f readline slapd You will be taken through an interactive dialog to set up your server. Answer the prompts with the same answers as below: Omit OpenLDAP server configuration? [yes/no] no DNS domain name: micro-hpc.org Organization name: micro-hpc Administrator password: test Confirm password: test Do you want your database to be removed when slapd is purged? [yes/no] yes Move old database? [yes/no] yes Note: For the password prompts, GNU readline will hide your inputs. Do not freak out when you do not see any characters appearing in the terminal when you are creating the adminstrator password.","title":"Enabling the LDAP server on ldap-0"},{"location":"ldap/#creating-user-test-and-group-research-on-the-server","text":"Still inside ldap-0 open a text editor window. I used nano in my case: $ nano add_test_user.ldif With the editor open, populate the file with the following LDIF ( LDAP Data Interchange Format ) content, and then save and close the file: dn: ou=People,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: People dn: ou=Groups,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: Groups dn: uid=test,ou=People,dc=micro-hpc,dc=org uid: test objectClass: inetOrgPerson objectClass: posixAccount cn: Test sn: Test givenName: Test mail: test@example.com userPassword: test uidNumber: 10000 gidNumber: 10000 loginShell: /bin/bash homeDirectory: /home/test dn: cn=test,ou=Groups,dc=micro-hpc,dc=org cn: test objectClass: posixGroup gidNumber: 10000 memberUid: nucci dn: cn=research,ou=Groups,dc=micro-hpc,dc=org cn: research objectClass: posixGroup gidNumber: 10100 memberUid: test Important: Make sure the content in your LDIF file is exactly the same as the code block above. Now use the following command to add user test and group research to the OpenLDAP server: ~# ldapadd -x -D \"cn=admin,dc=micro-hpc,dc=org\" -w test -f /root/add_test_user.ldif -H ldap:///","title":"Creating user test and group research on the server"},{"location":"ldap/#letting-everyone-else-know-about-user-test","text":"Now we need to set up all the other nodes so that they know about user test . To accomplish this, we will use the System Security Services Daemon , also known as SSSD. First, we need to grab the IPv4 address of the ldap-0 node. Execute the following command on your system. Note that you will need to run this command in a terminal window outside ldap-0 : $ lxc list -c n4 -f compact | grep ldap The output from the above command should look similar to the following output: ldap-0 10.5.1.44 (eth0) With the IPv4 address of ldap-0 in hand, open a text editor window: $ nano sssd.conf Now populate the sssd.conf file with the following content: [sssd] config_file_version = 2 domains = micro-hpc.org [domain/micro-hpc.org] id_provider = ldap auth_provider = ldap ldap_uri = ldap://10.5.1.44 cache_credentials = True ldap_search_base = dc=micro-hpc,dc=org Important: You should replace where I have my IPv4 address for ldap-0 with the IPv4 adress of your ldap-0 node. We are almost there! One thing to note with SSSD is that it requires the sssd.conf file to have very specific access permissions. Also, these permissions need to be the same across all nodes connecting to the OpenLDAP server. Let us use some fancy bash scripting to make the set up a little easier on ourselves: $ nodes=( nfs-0 head-0 compute-0 ) $ for i in ${nodes[@]}; do lxc file push sssd.conf $i/etc/sssd/sssd.conf lxc exec $i -- chmod 0600 /etc/sssd/sssd.conf lxc exec $i -- chown root:root /etc/sssd/sssd.conf lxc exec $i -- pam-auth-update --enable mkhomedir lxc exec $i -- systemctl enable sssd lxc exec $i -- systemctl start sssd done This for loop will save us a lot of copy, pasting, and changing a couple characters. Now onto setting up our shared file system!","title":"Letting everyone else know about user test"},{"location":"lmod/","text":"TODO","title":"Where is the software?"},{"location":"lmod/#todo","text":"","title":"TODO"},{"location":"nfs/","text":"Sharing is caring Right now, all of the nodes in our micro-HPC cluster are relatively operating independantly operating of one another; a file created on one node is not shared amongst the other nodes. This makes it albeit impossible to have many of thousands of nodes operating on the same data set. Luckily, we have Network File System , also know as NFS, to help us sync our data across the cluster. Setting up user test 's directories and keys First, we need to set up user test 's directories and keys so they can use our micro-HPC cluster. Start a shell session on the nfs-0 node: $ lxc shell nfs-0 Now, inside nfs-0 , create a directory for test under /data . This is where test will store all their files and code needed for their research. They will also need a home directory once they finally log onto the system: ~# mkdir -p /data/test ~# mkdir -p /home/test ~# chown -R test:test /data/test ~# chown -R test:test /home/test ~# chmod 0755 /data ~# chmod -R 0750 /data/test ~# chmod -R 0740 /home/test ~# ln -s /data/test /home/test/data However, even though we created the directories needed by user test , they still have no way for actually logging in. We can get around this by setting up an ssh key that will allow test to log into the cluster via ssh. You will need two terminal windows; one for the shell session on nfs-0 and the other on your system. Inside nfs-0 , login as user test using the following command: sudo -i -u test As user test on nfs-0 , execute the following commands to set up an authorized_keys file: $ mkdir .ssh $ touch .ssh/authorized_keys Now in the session on your system, create a private/public ssh keypair: $ ssh-keygen -t rsa -b 4096 -f test_rsa -N '' -q $ cat test_rsa.pub Copy the output of cat test_rsa.pub and then execute the following command inside nfs-0 : echo '<copied_public_key>' >> .ssh/authorized_keys Note: Be sure to replace <copied_public_key> with the public key that you copied from your system. Make sure you keep the key wrapped by the single quotes! Configuring what is shared by nfs-0 With user test all set up on nfs-0 , now it is time to configure how NFS exports directories on your system. Open a text editor window using the following command nfs-0 , but make sure that you are logged in as user root rather than test : ~# nano /etc/exports Populate /etc/exports with the content below: /srv *(ro,sync,subtree_check) /home *(rw,sync,no_subtree_check) /data *(rw,sync,no_subtree_check,no_root_squash) /opt *(rw,sync,no_subtree_check,no_root_squash) Save and close the file and then start the NFS server: ~# systemctl enable nfs-kernel-server ~# systemctl start nfs-kernel-server ~# exportfs -a Mounting the shared directories With your NFS server all set to go, now it is time to mount the shared directories inside the instances that need to consume those directories. In our case, these nodes will be compute-0 and head-0 . To get started, grab the IPv4 address of nfs-0 using the following command: $ lxc list -c n4 -f compact | grep nfs Now to save ourselves some grief, let us use a bash for loop to mount the shared drives in both head-0 and compute-0 : $ nodes=( compute-0 head-0 ) $ NFS_SERVER_IP=10.5.1.120 $ for i in ${nodes[@]}; do lxc exec $i -- mount $NFS_SERVER_IP:/home /home lxc exec $i -- mount $NFS_SERVER_IP:/data /data lxc exec $i -- mount $NFS_SERVER_IP:/opt /opt done Now onto setting up our resource management software.","title":"Everybody gets some data"},{"location":"nfs/#sharing-is-caring","text":"Right now, all of the nodes in our micro-HPC cluster are relatively operating independantly operating of one another; a file created on one node is not shared amongst the other nodes. This makes it albeit impossible to have many of thousands of nodes operating on the same data set. Luckily, we have Network File System , also know as NFS, to help us sync our data across the cluster.","title":"Sharing is caring"},{"location":"nfs/#setting-up-user-tests-directories-and-keys","text":"First, we need to set up user test 's directories and keys so they can use our micro-HPC cluster. Start a shell session on the nfs-0 node: $ lxc shell nfs-0 Now, inside nfs-0 , create a directory for test under /data . This is where test will store all their files and code needed for their research. They will also need a home directory once they finally log onto the system: ~# mkdir -p /data/test ~# mkdir -p /home/test ~# chown -R test:test /data/test ~# chown -R test:test /home/test ~# chmod 0755 /data ~# chmod -R 0750 /data/test ~# chmod -R 0740 /home/test ~# ln -s /data/test /home/test/data However, even though we created the directories needed by user test , they still have no way for actually logging in. We can get around this by setting up an ssh key that will allow test to log into the cluster via ssh. You will need two terminal windows; one for the shell session on nfs-0 and the other on your system. Inside nfs-0 , login as user test using the following command: sudo -i -u test As user test on nfs-0 , execute the following commands to set up an authorized_keys file: $ mkdir .ssh $ touch .ssh/authorized_keys Now in the session on your system, create a private/public ssh keypair: $ ssh-keygen -t rsa -b 4096 -f test_rsa -N '' -q $ cat test_rsa.pub Copy the output of cat test_rsa.pub and then execute the following command inside nfs-0 : echo '<copied_public_key>' >> .ssh/authorized_keys Note: Be sure to replace <copied_public_key> with the public key that you copied from your system. Make sure you keep the key wrapped by the single quotes!","title":"Setting up user test's directories and keys"},{"location":"nfs/#configuring-what-is-shared-by-nfs-0","text":"With user test all set up on nfs-0 , now it is time to configure how NFS exports directories on your system. Open a text editor window using the following command nfs-0 , but make sure that you are logged in as user root rather than test : ~# nano /etc/exports Populate /etc/exports with the content below: /srv *(ro,sync,subtree_check) /home *(rw,sync,no_subtree_check) /data *(rw,sync,no_subtree_check,no_root_squash) /opt *(rw,sync,no_subtree_check,no_root_squash) Save and close the file and then start the NFS server: ~# systemctl enable nfs-kernel-server ~# systemctl start nfs-kernel-server ~# exportfs -a","title":"Configuring what is shared by nfs-0"},{"location":"nfs/#mounting-the-shared-directories","text":"With your NFS server all set to go, now it is time to mount the shared directories inside the instances that need to consume those directories. In our case, these nodes will be compute-0 and head-0 . To get started, grab the IPv4 address of nfs-0 using the following command: $ lxc list -c n4 -f compact | grep nfs Now to save ourselves some grief, let us use a bash for loop to mount the shared drives in both head-0 and compute-0 : $ nodes=( compute-0 head-0 ) $ NFS_SERVER_IP=10.5.1.120 $ for i in ${nodes[@]}; do lxc exec $i -- mount $NFS_SERVER_IP:/home /home lxc exec $i -- mount $NFS_SERVER_IP:/data /data lxc exec $i -- mount $NFS_SERVER_IP:/opt /opt done Now onto setting up our resource management software.","title":"Mounting the shared directories"},{"location":"slurm/","text":"Workload Manager? Who needs it? You do! Imagine if a thousand people requested all request resources at the same time. Without any way to delegate responsibility across all the nodes in cluster, it would be a complete dumpster fire. Nodes with 1 TB of RAM would be going to tasks that maybe only need a few megabytes. Folks would get upset very fast; no one wants to own the cluster with so much promise but wasted potential. To avoid this problem, we will be using SLURM , also know as Simple Linux Utility for Resource Management . Setting up authentication with MUNGE Before we can use SLURM on our cluster, we need to set up MUNGE , SLURM's companion authentication program. They MUNGE key file munge.key needs to be exactly the same across all nodes being managed by SLURM. To synchronize the keys, first download munge.key from head-0 : $ lxc file pull head-0/etc/munge/munge.key With the munge.key file downloaded from head-0 , use the following commands to set up the key file on compute-0 : $ lxc file push munge.key compute-0/etc/munge/munge.key $ lxc exec compute-0 -- chown munge:munge /etc/munge/munge.key $ lxc exec compute-0 -- chmod 0600 /etc/munge/munge.key Now start the MUNGE authentication services on both compute-0 and head-0 : $ lxc exec compute-0 -- systemctl enable munge $ lxc exec compute-0 -- systemctl start munge $ lxc exec head-0 -- systemctl enable munge $ lxc exec head-0 -- systemctl start munge Getting groovy with node configuration Now that we have MUNGE set up, grab the IPv4 address of both compute-0 and head-0 using the following command: $ lxc list -c n4 -f compact | grep -E \"compute|head\" The output will look something similar to the output below: compute-0 10.5.1.149 (eth0) head-0 10.5.1.66 (eth0) Now on your system, open a text editor window in your terminal: $ nano slurm.conf Populate the file slurm.conf with the following information: SlurmctldHost=head-0(10.5.1.66) ClusterName=micro-hpc AuthType=auth/munge FirstJobId=65536 InactiveLimit=120 JobCompType=jobcomp/filetxt JobCompLoc=/var/log/slurm/jobcomp ProctrackType=proctrack/linuxproc KillWait=30 MaxJobCount=10000 MinJobAge=3600 ReturnToService=0 SchedulerType=sched/backfill SlurmctldLogFile=/var/log/slurm/slurmctld.log SlurmdLogFile=/var/log/slurm/slurmd.log SlurmctldPort=7002 SlurmdPort=7003 SlurmdSpoolDir=/var/spool/slurmd.spool StateSaveLocation=/var/spool/slurm.state SwitchType=switch/none TmpFS=/tmp WaitTime=30 # Node Configurations # NodeName=compute-0 NodeAddr=10.5.1.149 CPUs=1 RealMemory=2000 TmpDisk=10000 # Partition Configurations # PartitionName=all Nodes=compute-0 MaxTime=30 MaxNodes=3 State=UP Note: Ensure that you replace the IPv4 addresses I have above for both compute-0 and head-0 with the IPv4 addresses of the compute-0 and head-0 nodes on your system. Save and close the file and then use the following commands to upload the slurm.conf file to both compute-0 and head-0 : $ lxc file push slurm.conf compute-0/etc/slurm/slurm.conf $ lxc file push slurm.conf head-0/etc/slurm/slurm.conf Now use the following commands to start the slurmd service on compute-0 and the slurmctld service on head-0 : $ lxc exec compute-0 -- systemctl enable slurmd $ lxc exec compute-0 -- systemctl start slurmd $ lxc exec head-0 -- systemctl enable slurmctld $ lxc exec head-0 -- systemctl start slurmctld Now onto making our software stack!","title":"Bureaucracy and nodes"},{"location":"slurm/#workload-manager-who-needs-it","text":"You do! Imagine if a thousand people requested all request resources at the same time. Without any way to delegate responsibility across all the nodes in cluster, it would be a complete dumpster fire. Nodes with 1 TB of RAM would be going to tasks that maybe only need a few megabytes. Folks would get upset very fast; no one wants to own the cluster with so much promise but wasted potential. To avoid this problem, we will be using SLURM , also know as Simple Linux Utility for Resource Management .","title":"Workload Manager? Who needs it?"},{"location":"slurm/#setting-up-authentication-with-munge","text":"Before we can use SLURM on our cluster, we need to set up MUNGE , SLURM's companion authentication program. They MUNGE key file munge.key needs to be exactly the same across all nodes being managed by SLURM. To synchronize the keys, first download munge.key from head-0 : $ lxc file pull head-0/etc/munge/munge.key With the munge.key file downloaded from head-0 , use the following commands to set up the key file on compute-0 : $ lxc file push munge.key compute-0/etc/munge/munge.key $ lxc exec compute-0 -- chown munge:munge /etc/munge/munge.key $ lxc exec compute-0 -- chmod 0600 /etc/munge/munge.key Now start the MUNGE authentication services on both compute-0 and head-0 : $ lxc exec compute-0 -- systemctl enable munge $ lxc exec compute-0 -- systemctl start munge $ lxc exec head-0 -- systemctl enable munge $ lxc exec head-0 -- systemctl start munge","title":"Setting up authentication with MUNGE"},{"location":"slurm/#getting-groovy-with-node-configuration","text":"Now that we have MUNGE set up, grab the IPv4 address of both compute-0 and head-0 using the following command: $ lxc list -c n4 -f compact | grep -E \"compute|head\" The output will look something similar to the output below: compute-0 10.5.1.149 (eth0) head-0 10.5.1.66 (eth0) Now on your system, open a text editor window in your terminal: $ nano slurm.conf Populate the file slurm.conf with the following information: SlurmctldHost=head-0(10.5.1.66) ClusterName=micro-hpc AuthType=auth/munge FirstJobId=65536 InactiveLimit=120 JobCompType=jobcomp/filetxt JobCompLoc=/var/log/slurm/jobcomp ProctrackType=proctrack/linuxproc KillWait=30 MaxJobCount=10000 MinJobAge=3600 ReturnToService=0 SchedulerType=sched/backfill SlurmctldLogFile=/var/log/slurm/slurmctld.log SlurmdLogFile=/var/log/slurm/slurmd.log SlurmctldPort=7002 SlurmdPort=7003 SlurmdSpoolDir=/var/spool/slurmd.spool StateSaveLocation=/var/spool/slurm.state SwitchType=switch/none TmpFS=/tmp WaitTime=30 # Node Configurations # NodeName=compute-0 NodeAddr=10.5.1.149 CPUs=1 RealMemory=2000 TmpDisk=10000 # Partition Configurations # PartitionName=all Nodes=compute-0 MaxTime=30 MaxNodes=3 State=UP Note: Ensure that you replace the IPv4 addresses I have above for both compute-0 and head-0 with the IPv4 addresses of the compute-0 and head-0 nodes on your system. Save and close the file and then use the following commands to upload the slurm.conf file to both compute-0 and head-0 : $ lxc file push slurm.conf compute-0/etc/slurm/slurm.conf $ lxc file push slurm.conf head-0/etc/slurm/slurm.conf Now use the following commands to start the slurmd service on compute-0 and the slurmctld service on head-0 : $ lxc exec compute-0 -- systemctl enable slurmd $ lxc exec compute-0 -- systemctl start slurmd $ lxc exec head-0 -- systemctl enable slurmctld $ lxc exec head-0 -- systemctl start slurmctld Now onto making our software stack!","title":"Getting groovy with node configuration"},{"location":"spack/","text":"TODO","title":"Better software management with Spack"},{"location":"spack/#todo","text":"","title":"TODO"}]}