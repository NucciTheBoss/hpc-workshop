{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to NucciTheBoss's HPC workshop! If you are here for the workshop The open-source software behind High-Performance Computing , then you are in the right place! I am excited to give you an introduction to the open-source software that powers HPC on Ubuntu at the 2022 Ubuntu Summit in Prague. Forword While this workshop is set at the intermediate level, this workshop provides a beginner-friendly introduction to the open-source software behind HPC on Ubuntu. The reason this workshop is set to intermediate the level is because I expect you to be very comfortable working from the terminal on Ubuntu. If you are not familiar with executing complex bash commands, editing files, and/or managing multiple sessions from the terminal, then you may struggle with this workshop. I will be focusing on teaching folks how to build a pseudo-HPC cluster, not how to copy and paste text from the terminal. However , I was once a beginner too, so I will not be mean and stupid by leaving you in the dust. If you do not know how to do something and we have extra time, please call me over. I expect that you have a decent CPU, at least 4 cores, and at least 8 GB of RAM in your machine. You might be able to get away with lower specs, but things may take longer to \"finish.\" If your machine is struggling to keep up, try pairing with another individual and working together with them. I respect potatoes with a CPU, but there are just some things that they are not very good at... Now let us get on with the show!","title":"Introduction"},{"location":"#welcome-to-nuccithebosss-hpc-workshop","text":"If you are here for the workshop The open-source software behind High-Performance Computing , then you are in the right place! I am excited to give you an introduction to the open-source software that powers HPC on Ubuntu at the 2022 Ubuntu Summit in Prague.","title":"Welcome to NucciTheBoss's HPC workshop!"},{"location":"#forword","text":"While this workshop is set at the intermediate level, this workshop provides a beginner-friendly introduction to the open-source software behind HPC on Ubuntu. The reason this workshop is set to intermediate the level is because I expect you to be very comfortable working from the terminal on Ubuntu. If you are not familiar with executing complex bash commands, editing files, and/or managing multiple sessions from the terminal, then you may struggle with this workshop. I will be focusing on teaching folks how to build a pseudo-HPC cluster, not how to copy and paste text from the terminal. However , I was once a beginner too, so I will not be mean and stupid by leaving you in the dust. If you do not know how to do something and we have extra time, please call me over. I expect that you have a decent CPU, at least 4 cores, and at least 8 GB of RAM in your machine. You might be able to get away with lower specs, but things may take longer to \"finish.\" If your machine is struggling to keep up, try pairing with another individual and working together with them. I respect potatoes with a CPU, but there are just some things that they are not very good at... Now let us get on with the show!","title":"Forword"},{"location":"containers/","text":"Containers in HPC We can just use Docker, right? No. Docker is rarely used at all in HPC due to the security risks that it can impose upon the cluster. Someone with the right knowledge can go into an unhardened Docker container and quickly wreak havoc on the host system. To avoid these potential disasters, clusters use Apptainer instead. It is a read-only container system that only allows read operations, and you are the same user inside the container as outside the container. Building a simple container In our case, let us say that we have some ancient, mysterious COBOL code base that provides some critical calculations for your research. Rather than forcing everyone who tries to reproduce your results to beg their system administrator to install a COBOL compiler on the cluster, let us provide them a container they can pull instead. First, start a shell session on head-0 : $ lxc shell head-0 Now inside head-0 , log in an user test and open a text editor window. We will use this text editor window to create our \"ancient\" COBOL code base: ~# sudo -i -u test ~# nano ancient_code.cbl Populate the ancient_code.cbl file with the content below: IDENTIFICATION DIVISION. PROGRAM-ID. VARS. DATA DIVISION. WORKING-STORAGE SECTION. 01 FIRST-VAR PIC S9(3)V9(2) VALUE 445.62. 01 SECOND-VAR PIC S9(3)V9(2) VALUE -123.45. 01 THIRD-VAR PIC A(6) VALUE 'XYZ'. 01 FOURTH-VAR PIC X(5) VALUE 'M565$'. 01 GROUP-VAR. 05 SUBVAR-2 PIC X(6) VALUE 'ATOMS'. 05 SUBVAR-3 PIC X(10) VALUE 'CHEMICALS'. 05 SUBVAR-4 PIC X(6) VALUE 'OH MY'. PROCEDURE DIVISION. DISPLAY \"1ST RESULT : \" FIRST-VAR. DISPLAY \"2ND RESULT : \" SECOND-VAR. DISPLAY \"3RD RESULT : \" THIRD-VAR. DISPLAY \"4TH RESULT : \" FOURTH-VAR. DISPLAY \"SUMMARY: \" GROUP-VAR. STOP RUN. Important: Yes, this code file is formatted correctly. COBOL used to be written on punch cards where the first two \"tabs\" were dedicated to the line number. The styling convention was kept once COBOL migrated to being written on computers instead. Save and close after populating ancient_code.cbl with the \"ancient\" code. Now open a new text editor window: ~# nano cobol-runtime.def cobol-runtime.def will serve as the definition file for our container. With the definition file open, populate it with the content below: Bootstrap: docker From: ubuntu:22.04 %runscript /opt/bin/simulation %files ancient_code.cbl /opt/ancient_code.cbl %post apt-get update -y apt-get upgrade -y apt-get install -y gnucobol mkdir -p /opt/bin cobc -x -o /opt/bin/simulation /opt/ancient_code.cbl rm /opt/ancient_code.cbl %environment export PATH=/opt/bin:$PATH %help Encapsulate old COBOL in a flashy new container Save and close the definition file after you have populated the file, and then build an image using the following command: ~# apptainer build cobol-runtime.sif cobol-runtime.def Note: It will take a few minutes for the container to build. Using the container to run our program With the container image cobol-realtime.sif finally built, you can now run the simulation inside the apptainer image: ~# apptainer -s run cobol-runtime.sif If the container has built successfully, you should see the following output below: 1ST RESULT : +445.62 2ND RESULT : -123.45 3RD RESULT : XYZ 4TH RESULT : M565$ SUMMARY: ATOMS CHEMICALS OH MY Now onto adding more software with Spack!","title":"Getting fancy with containers"},{"location":"containers/#containers-in-hpc","text":"We can just use Docker, right? No. Docker is rarely used at all in HPC due to the security risks that it can impose upon the cluster. Someone with the right knowledge can go into an unhardened Docker container and quickly wreak havoc on the host system. To avoid these potential disasters, clusters use Apptainer instead. It is a read-only container system that only allows read operations, and you are the same user inside the container as outside the container.","title":"Containers in HPC"},{"location":"containers/#building-a-simple-container","text":"In our case, let us say that we have some ancient, mysterious COBOL code base that provides some critical calculations for your research. Rather than forcing everyone who tries to reproduce your results to beg their system administrator to install a COBOL compiler on the cluster, let us provide them a container they can pull instead. First, start a shell session on head-0 : $ lxc shell head-0 Now inside head-0 , log in an user test and open a text editor window. We will use this text editor window to create our \"ancient\" COBOL code base: ~# sudo -i -u test ~# nano ancient_code.cbl Populate the ancient_code.cbl file with the content below: IDENTIFICATION DIVISION. PROGRAM-ID. VARS. DATA DIVISION. WORKING-STORAGE SECTION. 01 FIRST-VAR PIC S9(3)V9(2) VALUE 445.62. 01 SECOND-VAR PIC S9(3)V9(2) VALUE -123.45. 01 THIRD-VAR PIC A(6) VALUE 'XYZ'. 01 FOURTH-VAR PIC X(5) VALUE 'M565$'. 01 GROUP-VAR. 05 SUBVAR-2 PIC X(6) VALUE 'ATOMS'. 05 SUBVAR-3 PIC X(10) VALUE 'CHEMICALS'. 05 SUBVAR-4 PIC X(6) VALUE 'OH MY'. PROCEDURE DIVISION. DISPLAY \"1ST RESULT : \" FIRST-VAR. DISPLAY \"2ND RESULT : \" SECOND-VAR. DISPLAY \"3RD RESULT : \" THIRD-VAR. DISPLAY \"4TH RESULT : \" FOURTH-VAR. DISPLAY \"SUMMARY: \" GROUP-VAR. STOP RUN. Important: Yes, this code file is formatted correctly. COBOL used to be written on punch cards where the first two \"tabs\" were dedicated to the line number. The styling convention was kept once COBOL migrated to being written on computers instead. Save and close after populating ancient_code.cbl with the \"ancient\" code. Now open a new text editor window: ~# nano cobol-runtime.def cobol-runtime.def will serve as the definition file for our container. With the definition file open, populate it with the content below: Bootstrap: docker From: ubuntu:22.04 %runscript /opt/bin/simulation %files ancient_code.cbl /opt/ancient_code.cbl %post apt-get update -y apt-get upgrade -y apt-get install -y gnucobol mkdir -p /opt/bin cobc -x -o /opt/bin/simulation /opt/ancient_code.cbl rm /opt/ancient_code.cbl %environment export PATH=/opt/bin:$PATH %help Encapsulate old COBOL in a flashy new container Save and close the definition file after you have populated the file, and then build an image using the following command: ~# apptainer build cobol-runtime.sif cobol-runtime.def Note: It will take a few minutes for the container to build.","title":"Building a simple container"},{"location":"containers/#using-the-container-to-run-our-program","text":"With the container image cobol-realtime.sif finally built, you can now run the simulation inside the apptainer image: ~# apptainer -s run cobol-runtime.sif If the container has built successfully, you should see the following output below: 1ST RESULT : +445.62 2ND RESULT : -123.45 3RD RESULT : XYZ 4TH RESULT : M565$ SUMMARY: ATOMS CHEMICALS OH MY Now onto adding more software with Spack!","title":"Using the container to run our program"},{"location":"future/","text":"What now? Congratulations! You have successfully built yourself a micro-HPC cluster. You might be asking yourself \"where to next?\" If you are interested in learning more about HPC in general on Ubuntu, feel free to check out our blog on the Ubuntu website: https://ubuntu.com/blog/tag/hpc If you are interested in learning more about how Canonical is involved in the HPC industry, you can check out our website: https://ubuntu.com/hpc If you are just interested in playing around with your cluster more, have a field day. You can do a lot of cool things on HPC clusters! Thank you for working through my workshop and I will see you again soon!","title":"Where to go from here?"},{"location":"future/#what-now","text":"Congratulations! You have successfully built yourself a micro-HPC cluster. You might be asking yourself \"where to next?\" If you are interested in learning more about HPC in general on Ubuntu, feel free to check out our blog on the Ubuntu website: https://ubuntu.com/blog/tag/hpc If you are interested in learning more about how Canonical is involved in the HPC industry, you can check out our website: https://ubuntu.com/hpc If you are just interested in playing around with your cluster more, have a field day. You can do a lot of cool things on HPC clusters! Thank you for working through my workshop and I will see you again soon!","title":"What now?"},{"location":"groundwork/","text":"So what will we be doing? We are going to be building an HPC cluster with LXD! Well... not really. We will be building a pseudo-HPC cluster, or as I like to call it, a micro-HPC cluster . I got the idea for the name from another popular project at Canonical: Microstack . True HPC clusters can fill an entire building; ours will just be a few LXD containers on your laptop. Setting up LXD on your system LXD will serve as the under-cloud for our HPC cluster. If you do not already have LXD installed on your system, use the following command to install the LXD snap package: $ sudo snap install lxd With the snap installed, set up LXD on your system with the following configuration options: $ lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: micro-hpc Name of the storage backend to use (lvm, zfs, ceph, btrfs, dir) [default=zfs]: Create a new ZFS pool? (yes/no) [default=yes]: Would you like to use an existing empty block device (e.g. a disk or partition)? (yes/no) [default=no]: Size in GB of the new loop device (1GB minimum) [default=27GB]: 60GB Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: Would you like the LXD server to be available over the network? (yes/no) [default=no]: Would you like stale cached images to be updated automatically? (yes/no) [default=yes]: Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: Getting the hpc-workshop pip package To make things go a \"faster\", I have written a small program to handle setting up what you will need for the micro-HPC cluster to work. It can be installed as pip package from PyPI: $ pip install hpc-workshop After installing the package, you should be able to access the hpc-workshop command. Note: You may have to run the command export PATH=$HOME/.local/bin:$PATH to access the hpc-workshop executable. Bootstrapping your cluster With hpc-workshop installed, all you to do is execute the following command: $ hpc-workshop init Yes, really, it is that simple! Now onto creating our cluster's user.","title":"Groundwork"},{"location":"groundwork/#so-what-will-we-be-doing","text":"We are going to be building an HPC cluster with LXD! Well... not really. We will be building a pseudo-HPC cluster, or as I like to call it, a micro-HPC cluster . I got the idea for the name from another popular project at Canonical: Microstack . True HPC clusters can fill an entire building; ours will just be a few LXD containers on your laptop.","title":"So what will we be doing?"},{"location":"groundwork/#setting-up-lxd-on-your-system","text":"LXD will serve as the under-cloud for our HPC cluster. If you do not already have LXD installed on your system, use the following command to install the LXD snap package: $ sudo snap install lxd With the snap installed, set up LXD on your system with the following configuration options: $ lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: micro-hpc Name of the storage backend to use (lvm, zfs, ceph, btrfs, dir) [default=zfs]: Create a new ZFS pool? (yes/no) [default=yes]: Would you like to use an existing empty block device (e.g. a disk or partition)? (yes/no) [default=no]: Size in GB of the new loop device (1GB minimum) [default=27GB]: 60GB Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: Would you like the LXD server to be available over the network? (yes/no) [default=no]: Would you like stale cached images to be updated automatically? (yes/no) [default=yes]: Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]:","title":"Setting up LXD on your system"},{"location":"groundwork/#getting-the-hpc-workshop-pip-package","text":"To make things go a \"faster\", I have written a small program to handle setting up what you will need for the micro-HPC cluster to work. It can be installed as pip package from PyPI: $ pip install hpc-workshop After installing the package, you should be able to access the hpc-workshop command. Note: You may have to run the command export PATH=$HOME/.local/bin:$PATH to access the hpc-workshop executable.","title":"Getting the hpc-workshop pip package"},{"location":"groundwork/#bootstrapping-your-cluster","text":"With hpc-workshop installed, all you to do is execute the following command: $ hpc-workshop init Yes, really, it is that simple! Now onto creating our cluster's user.","title":"Bootstrapping your cluster"},{"location":"ldap/","text":"Who is going to use our cluster? Obviously, we cannot let root be the sole user of our cluster - that would create a disaster. Therefore, to create a user, and have that user exists across all nodes within our micro-HPC cluster. To accomplish this, we will use OpenLDAP , an open-source implementation of the Lightweight Directory Access Protocol (LDAP). Enabling the LDAP server on ldap-0 Before we can add our user to the cluster, we need to start the OpenLDAP server. Let us start a shell session inside the ldap-0 node: $ lxc shell ldap-0 Now inside the ldap-0 node, execute the following commands to start the OpenLDAP server: ~# systemctl enable slapd ~# systemctl start slapd We are not done yet, however, for we need to also configure the OpenLDAP server. Luckily, dpkg-reconfigure can handle most of the legwork for us: ~# dpkg-reconfigure -f readline slapd You will be taken through an interactive dialog to set up your server. Answer the prompts with the same answers as below: Omit OpenLDAP server configuration? [yes/no] no DNS domain name: micro-hpc.org Organization name: micro-hpc Administrator password: test Confirm password: test Do you want your database to be removed when slapd is purged? [yes/no] yes Move old database? [yes/no] yes Note: For the password prompts, GNU readline will hide your inputs. Do not freak out when you do not see any characters appearing in the terminal when you are creating the administrator password. Creating user test and group research on the server Still inside ldap-0 open a text editor window. I used nano in my case: $ nano add_test_user.ldif With the editor open, populate the file with the following LDIF ( LDAP Data Interchange Format ) content, and then save and close the file: dn: ou=People,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: People dn: ou=Groups,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: Groups dn: uid=test,ou=People,dc=micro-hpc,dc=org uid: test objectClass: inetOrgPerson objectClass: posixAccount cn: Test sn: Test givenName: Test mail: test@example.com userPassword: test uidNumber: 10000 gidNumber: 10000 loginShell: /bin/bash homeDirectory: /home/test dn: cn=test,ou=Groups,dc=micro-hpc,dc=org cn: test objectClass: posixGroup gidNumber: 10000 memberUid: nucci dn: cn=research,ou=Groups,dc=micro-hpc,dc=org cn: research objectClass: posixGroup gidNumber: 10100 memberUid: test Important: Make sure the content in your LDIF file is exactly the same as the code block above. Now use the following command to add user test and group research to the OpenLDAP server: ~# ldapadd -x -D \"cn=admin,dc=micro-hpc,dc=org\" -w test -f /root/add_test_user.ldif -H ldap:/// Letting everyone else know about user test Now we need to set up all the other nodes so that they know about user test . To accomplish this, we will use the System Security Services Daemon , also known as SSSD. First, we need to grab the IPv4 address of the ldap-0 node. Execute the following command on your system. Note that you will need to run this command in a terminal window outside ldap-0 : $ lxc list -c n4 -f compact | grep ldap The output from the above command should look similar to the following output: ldap-0 10.5.1.44 (eth0) With the IPv4 address of ldap-0 in hand, open a text editor window: $ nano sssd.conf Now populate the sssd.conf file with the following content: [sssd] config_file_version = 2 domains = micro-hpc.org [domain/micro-hpc.org] id_provider = ldap auth_provider = ldap ldap_uri = ldap://10.5.1.44 cache_credentials = True ldap_search_base = dc=micro-hpc,dc=org Important: You should replace where I have my IPv4 address for ldap-0 with the IPv4 adress of your ldap-0 node. We are almost there! One thing to note with SSSD is that it requires the sssd.conf file to have very specific access permissions. Also, these permissions need to be the same across all nodes connecting to the OpenLDAP server. Let us use some fancy bash scripting to make the set up a little easier on ourselves: $ nodes=( nfs-0 head-0 compute-0 ) $ for i in ${nodes[@]}; do lxc file push sssd.conf $i/etc/sssd/sssd.conf lxc exec $i -- chmod 0600 /etc/sssd/sssd.conf lxc exec $i -- chown root:root /etc/sssd/sssd.conf lxc exec $i -- pam-auth-update --enable mkhomedir lxc exec $i -- systemctl enable sssd lxc exec $i -- systemctl start sssd done This for loop will save us a lot of copy, pasting, and changing a couple characters. Now onto setting up our shared file system!","title":"Setting up our researcher"},{"location":"ldap/#who-is-going-to-use-our-cluster","text":"Obviously, we cannot let root be the sole user of our cluster - that would create a disaster. Therefore, to create a user, and have that user exists across all nodes within our micro-HPC cluster. To accomplish this, we will use OpenLDAP , an open-source implementation of the Lightweight Directory Access Protocol (LDAP).","title":"Who is going to use our cluster?"},{"location":"ldap/#enabling-the-ldap-server-on-ldap-0","text":"Before we can add our user to the cluster, we need to start the OpenLDAP server. Let us start a shell session inside the ldap-0 node: $ lxc shell ldap-0 Now inside the ldap-0 node, execute the following commands to start the OpenLDAP server: ~# systemctl enable slapd ~# systemctl start slapd We are not done yet, however, for we need to also configure the OpenLDAP server. Luckily, dpkg-reconfigure can handle most of the legwork for us: ~# dpkg-reconfigure -f readline slapd You will be taken through an interactive dialog to set up your server. Answer the prompts with the same answers as below: Omit OpenLDAP server configuration? [yes/no] no DNS domain name: micro-hpc.org Organization name: micro-hpc Administrator password: test Confirm password: test Do you want your database to be removed when slapd is purged? [yes/no] yes Move old database? [yes/no] yes Note: For the password prompts, GNU readline will hide your inputs. Do not freak out when you do not see any characters appearing in the terminal when you are creating the administrator password.","title":"Enabling the LDAP server on ldap-0"},{"location":"ldap/#creating-user-test-and-group-research-on-the-server","text":"Still inside ldap-0 open a text editor window. I used nano in my case: $ nano add_test_user.ldif With the editor open, populate the file with the following LDIF ( LDAP Data Interchange Format ) content, and then save and close the file: dn: ou=People,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: People dn: ou=Groups,dc=micro-hpc,dc=org objectClass: organizationalUnit ou: Groups dn: uid=test,ou=People,dc=micro-hpc,dc=org uid: test objectClass: inetOrgPerson objectClass: posixAccount cn: Test sn: Test givenName: Test mail: test@example.com userPassword: test uidNumber: 10000 gidNumber: 10000 loginShell: /bin/bash homeDirectory: /home/test dn: cn=test,ou=Groups,dc=micro-hpc,dc=org cn: test objectClass: posixGroup gidNumber: 10000 memberUid: nucci dn: cn=research,ou=Groups,dc=micro-hpc,dc=org cn: research objectClass: posixGroup gidNumber: 10100 memberUid: test Important: Make sure the content in your LDIF file is exactly the same as the code block above. Now use the following command to add user test and group research to the OpenLDAP server: ~# ldapadd -x -D \"cn=admin,dc=micro-hpc,dc=org\" -w test -f /root/add_test_user.ldif -H ldap:///","title":"Creating user test and group research on the server"},{"location":"ldap/#letting-everyone-else-know-about-user-test","text":"Now we need to set up all the other nodes so that they know about user test . To accomplish this, we will use the System Security Services Daemon , also known as SSSD. First, we need to grab the IPv4 address of the ldap-0 node. Execute the following command on your system. Note that you will need to run this command in a terminal window outside ldap-0 : $ lxc list -c n4 -f compact | grep ldap The output from the above command should look similar to the following output: ldap-0 10.5.1.44 (eth0) With the IPv4 address of ldap-0 in hand, open a text editor window: $ nano sssd.conf Now populate the sssd.conf file with the following content: [sssd] config_file_version = 2 domains = micro-hpc.org [domain/micro-hpc.org] id_provider = ldap auth_provider = ldap ldap_uri = ldap://10.5.1.44 cache_credentials = True ldap_search_base = dc=micro-hpc,dc=org Important: You should replace where I have my IPv4 address for ldap-0 with the IPv4 adress of your ldap-0 node. We are almost there! One thing to note with SSSD is that it requires the sssd.conf file to have very specific access permissions. Also, these permissions need to be the same across all nodes connecting to the OpenLDAP server. Let us use some fancy bash scripting to make the set up a little easier on ourselves: $ nodes=( nfs-0 head-0 compute-0 ) $ for i in ${nodes[@]}; do lxc file push sssd.conf $i/etc/sssd/sssd.conf lxc exec $i -- chmod 0600 /etc/sssd/sssd.conf lxc exec $i -- chown root:root /etc/sssd/sssd.conf lxc exec $i -- pam-auth-update --enable mkhomedir lxc exec $i -- systemctl enable sssd lxc exec $i -- systemctl start sssd done This for loop will save us a lot of copy, pasting, and changing a couple characters. Now onto setting up our shared file system!","title":"Letting everyone else know about user test"},{"location":"lmod/","text":"Put a pin in using debs or snaps On HPC clusters, there are typically thousands if not tens of thousands of daily users who all have their own custom software stacks. Reproducibility crucial in scientific research, even down to the point of having the same version of the programming language. Updating the software stack on a whim quickly leads to a bunch of unhappy researchers. Luckily, we have some nice software to help us manage everyone's custom software deployment: Lmod ! Lmod is a Lua-based module system that enables you to dynamically load software as defined in Lua module files. Creating a Lua module file If you look in the /opt/sw directory on nfs-0 , you will see the directory apptainer : ~# ls /opt/sw This is the first program on our cluster that we will make available to our user test . To get started, open a text editor window using the following command: ~# nano 1.1.3.lua Inside the text editor window, populate the 1.1.3.lua file with the content below: help([[Apptainer is a container management software for HPC gurus like you and me]]) local root = \"/opt/sw/apptainer\" app_bin = pathJoin(root, \"bin\") app_libexec_bin = pathJoin(root, \"libexec/apptainer/bin\") app_libexec_cni = pathJoin(root, \"libexec/apptainer/cni\") app_libexec_lib = pathJoin(root, \"libexec/apptainer/lib\") app_man = pathJoin(root, \"share/man\") prepend_path(\"PATH\", app_bin) prepend_path(\"PATH\", app_libexec_bin) prepend_path(\"PATH\", app_libexec_cni) prepend_path(\"LIBRARY_PATH\", app_libexec_lib) prepend_path(\"LD_LIBRARY_PATH\", app_libexec_lib) prepend_path(\"MANPATH\", app_man) Save and close the file once you are done populating the file. Setting up the module file To set up the module file up on nfs-0 , use the following commands: ~# echo /opt/sw/modules > /opt/apps/lmod/lmod/init/.modulespath ~# mkdir -p /opt/sw/modules/apptainer ~# mv 1.1.3.lua /opt/sw/modules/apptainer Accessing apptainer With the module file set in place, let us test that it works. Log into nfs-0 as user test , and then load the apptainer module: ~# sudo -i -u test $ module load apptainer To test that apptainer works, use the following command to print the version number: $ apptainer version If you see the version number printed out to the terminal, then it is time to move on to building a container image with apptainer!","title":"Where is the software?"},{"location":"lmod/#put-a-pin-in-using-debs-or-snaps","text":"On HPC clusters, there are typically thousands if not tens of thousands of daily users who all have their own custom software stacks. Reproducibility crucial in scientific research, even down to the point of having the same version of the programming language. Updating the software stack on a whim quickly leads to a bunch of unhappy researchers. Luckily, we have some nice software to help us manage everyone's custom software deployment: Lmod ! Lmod is a Lua-based module system that enables you to dynamically load software as defined in Lua module files.","title":"Put a pin in using debs or snaps"},{"location":"lmod/#creating-a-lua-module-file","text":"If you look in the /opt/sw directory on nfs-0 , you will see the directory apptainer : ~# ls /opt/sw This is the first program on our cluster that we will make available to our user test . To get started, open a text editor window using the following command: ~# nano 1.1.3.lua Inside the text editor window, populate the 1.1.3.lua file with the content below: help([[Apptainer is a container management software for HPC gurus like you and me]]) local root = \"/opt/sw/apptainer\" app_bin = pathJoin(root, \"bin\") app_libexec_bin = pathJoin(root, \"libexec/apptainer/bin\") app_libexec_cni = pathJoin(root, \"libexec/apptainer/cni\") app_libexec_lib = pathJoin(root, \"libexec/apptainer/lib\") app_man = pathJoin(root, \"share/man\") prepend_path(\"PATH\", app_bin) prepend_path(\"PATH\", app_libexec_bin) prepend_path(\"PATH\", app_libexec_cni) prepend_path(\"LIBRARY_PATH\", app_libexec_lib) prepend_path(\"LD_LIBRARY_PATH\", app_libexec_lib) prepend_path(\"MANPATH\", app_man) Save and close the file once you are done populating the file.","title":"Creating a Lua module file"},{"location":"lmod/#setting-up-the-module-file","text":"To set up the module file up on nfs-0 , use the following commands: ~# echo /opt/sw/modules > /opt/apps/lmod/lmod/init/.modulespath ~# mkdir -p /opt/sw/modules/apptainer ~# mv 1.1.3.lua /opt/sw/modules/apptainer","title":"Setting up the module file"},{"location":"lmod/#accessing-apptainer","text":"With the module file set in place, let us test that it works. Log into nfs-0 as user test , and then load the apptainer module: ~# sudo -i -u test $ module load apptainer To test that apptainer works, use the following command to print the version number: $ apptainer version If you see the version number printed out to the terminal, then it is time to move on to building a container image with apptainer!","title":"Accessing apptainer"},{"location":"nfs/","text":"Sharing is caring Right now, all of the nodes in our micro-HPC cluster are relatively operating independently operating of one another; a file created on one node is not shared amongst the other nodes. This makes it difficult impossible to have many of thousands of nodes operating on the same data set. Luckily, we have Network File System , also known as NFS, to help us sync our data across the cluster. Setting up user test 's directories First, we need to set up user test 's directories and keys so they can use our micro-HPC cluster. Start a shell session on the nfs-0 node: $ lxc shell nfs-0 Now, inside nfs-0 , create a directory for test under /data . This is where test will store all their files and code needed for their research. They will also need a home directory once they finally log onto the system: ~# mkdir -p /data/test ~# mkdir -p /home/test ~# chown -R test:test /data/test ~# chown -R test:test /home/test ~# chmod 0755 /data ~# chmod -R 0750 /data/test ~# chmod -R 0740 /home/test ~# ln -s /data/test /home/test/data Configuring what is shared by nfs-0 With user test all set up on nfs-0 , now it is time to configure how NFS exports directories on your system. Open a text editor window using the following command nfs-0 , but make sure that you are logged in as user root rather than test : ~# nano /etc/exports Populate /etc/exports with the content below: /srv *(ro,sync,subtree_check) /home *(rw,sync,no_subtree_check) /data *(rw,sync,no_subtree_check,no_root_squash) /opt *(rw,sync,no_subtree_check,no_root_squash) Save and close the file and then start the NFS server: ~# systemctl enable nfs-kernel-server ~# systemctl start nfs-kernel-server ~# exportfs -a Mounting the shared directories With your NFS server all set to go, now it is time to mount the shared directories inside the instances that need to consume those directories. In our case, these nodes will be compute-0 and head-0 . To get started, grab the IPv4 address of nfs-0 using the following command: $ lxc list -c n4 -f compact | grep nfs Now to save ourselves some grief, let us use a bash for loop to mount the shared drives in both head-0 and compute-0 : $ nodes=( compute-0 head-0 ) $ NFS_SERVER_IP=10.5.1.120 $ for i in ${nodes[@]}; do lxc exec $i -- mount $NFS_SERVER_IP:/home /home lxc exec $i -- mount $NFS_SERVER_IP:/data /data lxc exec $i -- mount $NFS_SERVER_IP:/opt /opt done Now onto setting up our resource management software.","title":"Everybody gets some data"},{"location":"nfs/#sharing-is-caring","text":"Right now, all of the nodes in our micro-HPC cluster are relatively operating independently operating of one another; a file created on one node is not shared amongst the other nodes. This makes it difficult impossible to have many of thousands of nodes operating on the same data set. Luckily, we have Network File System , also known as NFS, to help us sync our data across the cluster.","title":"Sharing is caring"},{"location":"nfs/#setting-up-user-tests-directories","text":"First, we need to set up user test 's directories and keys so they can use our micro-HPC cluster. Start a shell session on the nfs-0 node: $ lxc shell nfs-0 Now, inside nfs-0 , create a directory for test under /data . This is where test will store all their files and code needed for their research. They will also need a home directory once they finally log onto the system: ~# mkdir -p /data/test ~# mkdir -p /home/test ~# chown -R test:test /data/test ~# chown -R test:test /home/test ~# chmod 0755 /data ~# chmod -R 0750 /data/test ~# chmod -R 0740 /home/test ~# ln -s /data/test /home/test/data","title":"Setting up user test's directories"},{"location":"nfs/#configuring-what-is-shared-by-nfs-0","text":"With user test all set up on nfs-0 , now it is time to configure how NFS exports directories on your system. Open a text editor window using the following command nfs-0 , but make sure that you are logged in as user root rather than test : ~# nano /etc/exports Populate /etc/exports with the content below: /srv *(ro,sync,subtree_check) /home *(rw,sync,no_subtree_check) /data *(rw,sync,no_subtree_check,no_root_squash) /opt *(rw,sync,no_subtree_check,no_root_squash) Save and close the file and then start the NFS server: ~# systemctl enable nfs-kernel-server ~# systemctl start nfs-kernel-server ~# exportfs -a","title":"Configuring what is shared by nfs-0"},{"location":"nfs/#mounting-the-shared-directories","text":"With your NFS server all set to go, now it is time to mount the shared directories inside the instances that need to consume those directories. In our case, these nodes will be compute-0 and head-0 . To get started, grab the IPv4 address of nfs-0 using the following command: $ lxc list -c n4 -f compact | grep nfs Now to save ourselves some grief, let us use a bash for loop to mount the shared drives in both head-0 and compute-0 : $ nodes=( compute-0 head-0 ) $ NFS_SERVER_IP=10.5.1.120 $ for i in ${nodes[@]}; do lxc exec $i -- mount $NFS_SERVER_IP:/home /home lxc exec $i -- mount $NFS_SERVER_IP:/data /data lxc exec $i -- mount $NFS_SERVER_IP:/opt /opt done Now onto setting up our resource management software.","title":"Mounting the shared directories"},{"location":"slurm/","text":"Workload Manager? Who needs it? You do! Imagine if a thousand people requested all request resources at the same time. Without any way to delegate responsibility across all the nodes in cluster, it would be a complete dumpster fire. Nodes with 1 TB of RAM would be going to tasks that maybe only need a few megabytes. Folks would get upset very fast; no one wants to own the cluster with so much promise but wasted potential. To avoid this problem, we will be using SLURM , also know as Simple Linux Utility for Resource Management . Setting up authentication with MUNGE Before we can use SLURM on our cluster, we need to set up MUNGE , SLURM's companion authentication program. They MUNGE key file munge.key needs to be exactly the same across all nodes being managed by SLURM. To synchronize the keys, first download munge.key from head-0 : $ lxc file pull head-0/etc/munge/munge.key munge.key With the munge.key file downloaded from head-0 , use the following commands to set up the key file on compute-0 : $ lxc file push munge.key compute-0/etc/munge/munge.key $ lxc exec compute-0 -- chown munge:munge /etc/munge/munge.key $ lxc exec compute-0 -- chmod 0600 /etc/munge/munge.key Now start the MUNGE authentication services on both compute-0 and head-0 : $ lxc exec compute-0 -- systemctl enable munge $ lxc exec compute-0 -- systemctl start munge $ lxc exec head-0 -- systemctl enable munge $ lxc exec head-0 -- systemctl start munge Getting groovy with node configuration Now that we have MUNGE set up, grab the IPv4 address of both compute-0 and head-0 using the following command: $ lxc list -c n4 -f compact | grep -E \"compute|head\" The output will look something similar to the output below: compute-0 10.5.1.149 (eth0) head-0 10.5.1.66 (eth0) Now on your system, open a text editor window in your terminal: $ nano slurm.conf Populate the file slurm.conf with the following information: SlurmctldHost=head-0(10.5.1.66) ClusterName=micro-hpc AuthType=auth/munge FirstJobId=65536 InactiveLimit=120 JobCompType=jobcomp/filetxt JobCompLoc=/var/log/slurm/jobcomp ProctrackType=proctrack/linuxproc KillWait=30 MaxJobCount=10000 MinJobAge=3600 ReturnToService=0 SchedulerType=sched/backfill SlurmctldLogFile=/var/log/slurm/slurmctld.log SlurmdLogFile=/var/log/slurm/slurmd.log SlurmctldPort=7002 SlurmdPort=7003 SlurmdSpoolDir=/var/spool/slurmd.spool StateSaveLocation=/var/spool/slurm.state SwitchType=switch/none TmpFS=/tmp WaitTime=30 # Node Configurations # NodeName=compute-0 NodeAddr=10.5.1.149 CPUs=1 RealMemory=1000 TmpDisk=10000 # Partition Configurations # PartitionName=all Nodes=compute-0 MaxTime=30 MaxNodes=3 State=UP Note: Ensure that you replace the IPv4 addresses I have above for both compute-0 and head-0 with the IPv4 addresses of the compute-0 and head-0 nodes on your system. Save and close the file and then use the following commands to upload the slurm.conf file to both compute-0 and head-0 : $ lxc file push slurm.conf compute-0/etc/slurm/slurm.conf $ lxc file push slurm.conf head-0/etc/slurm/slurm.conf Now use the following commands to start the slurmd service on compute-0 and the slurmctld service on head-0 : $ lxc exec compute-0 -- systemctl enable slurmd $ lxc exec compute-0 -- systemctl start slurmd $ lxc exec head-0 -- systemctl enable slurmctld $ lxc exec head-0 -- systemctl start slurmctld Now onto making our software stack!","title":"Bureaucracy and nodes"},{"location":"slurm/#workload-manager-who-needs-it","text":"You do! Imagine if a thousand people requested all request resources at the same time. Without any way to delegate responsibility across all the nodes in cluster, it would be a complete dumpster fire. Nodes with 1 TB of RAM would be going to tasks that maybe only need a few megabytes. Folks would get upset very fast; no one wants to own the cluster with so much promise but wasted potential. To avoid this problem, we will be using SLURM , also know as Simple Linux Utility for Resource Management .","title":"Workload Manager? Who needs it?"},{"location":"slurm/#setting-up-authentication-with-munge","text":"Before we can use SLURM on our cluster, we need to set up MUNGE , SLURM's companion authentication program. They MUNGE key file munge.key needs to be exactly the same across all nodes being managed by SLURM. To synchronize the keys, first download munge.key from head-0 : $ lxc file pull head-0/etc/munge/munge.key munge.key With the munge.key file downloaded from head-0 , use the following commands to set up the key file on compute-0 : $ lxc file push munge.key compute-0/etc/munge/munge.key $ lxc exec compute-0 -- chown munge:munge /etc/munge/munge.key $ lxc exec compute-0 -- chmod 0600 /etc/munge/munge.key Now start the MUNGE authentication services on both compute-0 and head-0 : $ lxc exec compute-0 -- systemctl enable munge $ lxc exec compute-0 -- systemctl start munge $ lxc exec head-0 -- systemctl enable munge $ lxc exec head-0 -- systemctl start munge","title":"Setting up authentication with MUNGE"},{"location":"slurm/#getting-groovy-with-node-configuration","text":"Now that we have MUNGE set up, grab the IPv4 address of both compute-0 and head-0 using the following command: $ lxc list -c n4 -f compact | grep -E \"compute|head\" The output will look something similar to the output below: compute-0 10.5.1.149 (eth0) head-0 10.5.1.66 (eth0) Now on your system, open a text editor window in your terminal: $ nano slurm.conf Populate the file slurm.conf with the following information: SlurmctldHost=head-0(10.5.1.66) ClusterName=micro-hpc AuthType=auth/munge FirstJobId=65536 InactiveLimit=120 JobCompType=jobcomp/filetxt JobCompLoc=/var/log/slurm/jobcomp ProctrackType=proctrack/linuxproc KillWait=30 MaxJobCount=10000 MinJobAge=3600 ReturnToService=0 SchedulerType=sched/backfill SlurmctldLogFile=/var/log/slurm/slurmctld.log SlurmdLogFile=/var/log/slurm/slurmd.log SlurmctldPort=7002 SlurmdPort=7003 SlurmdSpoolDir=/var/spool/slurmd.spool StateSaveLocation=/var/spool/slurm.state SwitchType=switch/none TmpFS=/tmp WaitTime=30 # Node Configurations # NodeName=compute-0 NodeAddr=10.5.1.149 CPUs=1 RealMemory=1000 TmpDisk=10000 # Partition Configurations # PartitionName=all Nodes=compute-0 MaxTime=30 MaxNodes=3 State=UP Note: Ensure that you replace the IPv4 addresses I have above for both compute-0 and head-0 with the IPv4 addresses of the compute-0 and head-0 nodes on your system. Save and close the file and then use the following commands to upload the slurm.conf file to both compute-0 and head-0 : $ lxc file push slurm.conf compute-0/etc/slurm/slurm.conf $ lxc file push slurm.conf head-0/etc/slurm/slurm.conf Now use the following commands to start the slurmd service on compute-0 and the slurmctld service on head-0 : $ lxc exec compute-0 -- systemctl enable slurmd $ lxc exec compute-0 -- systemctl start slurmd $ lxc exec head-0 -- systemctl enable slurmctld $ lxc exec head-0 -- systemctl start slurmctld Now onto making our software stack!","title":"Getting groovy with node configuration"},{"location":"spack/","text":"But I like having apt and snap Yes, apt and snap are excellent package managers for Ubuntu, however, they are not commonly used for distributing HPC software. Why? Because everyone has their own custom implementation that pre-built packages might not necessarily meet. For example, some researchers may opt to compile their dependencies with intel or pgi compilers rather than use GNU's compiler collection. As such, we need a package manager that allows researchers to role their own custom software stacks. Luckily, there are a couple of options for this with one of the most popular being Spack . Installing spack To install Spack, start a shell session inside head-0 and then log in as user test : $ lxc shell head-0 ~# sudo -i -u test Execute the following command as user test to install Spack inside their home directory: $ git clone -c feature.manyFiles=true https://github.com/spack/spack.git Note: Spack is a rather beefy git repository, so the clone may take a few minutes Once git has finished cloning the spack repository, execute the following command to set up inside user test 's environment: $ . spack/share/spack/setup-env.sh Compiling a package With spack set up inside user test 's environment, you can use the following command to install the cowsay package: $ spack install cowsay Note: Installing cowsay will also take a few minutes to install since spack installs almost every package from source. Making the package available to user test Once cowsay has finished installing, the following commands can be used to make cowsay accessible to user test : $ spack load cowsay $ cowsay 'HPC on LXD is great!' If cowsay was installed correctly, your cow should say HPC on LXD is great! in the terminal window: ______________________ < HPC on LXD is great! > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Now it is time to bring everything together and submit our first job!","title":"Better software management with Spack"},{"location":"spack/#but-i-like-having-apt-and-snap","text":"Yes, apt and snap are excellent package managers for Ubuntu, however, they are not commonly used for distributing HPC software. Why? Because everyone has their own custom implementation that pre-built packages might not necessarily meet. For example, some researchers may opt to compile their dependencies with intel or pgi compilers rather than use GNU's compiler collection. As such, we need a package manager that allows researchers to role their own custom software stacks. Luckily, there are a couple of options for this with one of the most popular being Spack .","title":"But I like having apt and snap"},{"location":"spack/#installing-spack","text":"To install Spack, start a shell session inside head-0 and then log in as user test : $ lxc shell head-0 ~# sudo -i -u test Execute the following command as user test to install Spack inside their home directory: $ git clone -c feature.manyFiles=true https://github.com/spack/spack.git Note: Spack is a rather beefy git repository, so the clone may take a few minutes Once git has finished cloning the spack repository, execute the following command to set up inside user test 's environment: $ . spack/share/spack/setup-env.sh","title":"Installing spack"},{"location":"spack/#compiling-a-package","text":"With spack set up inside user test 's environment, you can use the following command to install the cowsay package: $ spack install cowsay Note: Installing cowsay will also take a few minutes to install since spack installs almost every package from source.","title":"Compiling a package"},{"location":"spack/#making-the-package-available-to-user-test","text":"Once cowsay has finished installing, the following commands can be used to make cowsay accessible to user test : $ spack load cowsay $ cowsay 'HPC on LXD is great!' If cowsay was installed correctly, your cow should say HPC on LXD is great! in the terminal window: ______________________ < HPC on LXD is great! > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Now it is time to bring everything together and submit our first job!","title":"Making the package available to user test"},{"location":"together/","text":"Time to enjoy the fruits of your labor We have gone through all this work to set up our cluster, so now it is time for us to enjoy the fruits of our labor. We are going to use all the software we have set up to submit our first job. Writing a job Log into the head-0 as user test : $ lxc shell head-0 ~# sudo -i -u nucci Logged in as user test , open a text editor window: $ nano research.submit With the file research.submit open, populate the file with the content below: #!/bin/bash #SBATCH --job-name=research #SBATCH --partition=all #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=500mb #SBATCH --time=00:00:30 #SBATCH --error=research.%J.err #SBATCH --output=research.%J.out . spack/share/spack/setup-env.sh spack load cowsay module load apptainer apptainer -s run cobol-runtime.sif echo -e \"\\n\\n\" cowsay 'Done' Save and close the file once you are done populating the research.job file. Important: If you notice that your SLURM compute node has the state of drained, you will need to undrain it. You can undrain the node by launching the scontrol interpreter and issuing the following commands: text ~# scontrol update NodeName=compute-0 State=DOWN Reason=\"undraining\" update NodeName=compute-0 State=RESUME Submitting a job You can use the following command to submit your job file to your micro-HPC cluster: $ sbatch research.submit Evaluating the results After a few seconds, your job's results should be returned by SLURM. You can use the following command to browse through the results of your job: $ less research.65539.out Note: Your job ID number may defer from the one above. You should see something like the following in your less window: 1ST RESULT : +445.62 2ND RESULT : -123.45 3RD RESULT : XYZ 4TH RESULT : M565$ SUMMARY: ATOMS CHEMICALS OH MY ______ < Done > ------ \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Congratulations! You successfully ran your first job. This is the end of the workshop, so the next page will discuss where you can go from here!","title":"Bringing it all together"},{"location":"together/#time-to-enjoy-the-fruits-of-your-labor","text":"We have gone through all this work to set up our cluster, so now it is time for us to enjoy the fruits of our labor. We are going to use all the software we have set up to submit our first job.","title":"Time to enjoy the fruits of your labor"},{"location":"together/#writing-a-job","text":"Log into the head-0 as user test : $ lxc shell head-0 ~# sudo -i -u nucci Logged in as user test , open a text editor window: $ nano research.submit With the file research.submit open, populate the file with the content below: #!/bin/bash #SBATCH --job-name=research #SBATCH --partition=all #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=500mb #SBATCH --time=00:00:30 #SBATCH --error=research.%J.err #SBATCH --output=research.%J.out . spack/share/spack/setup-env.sh spack load cowsay module load apptainer apptainer -s run cobol-runtime.sif echo -e \"\\n\\n\" cowsay 'Done' Save and close the file once you are done populating the research.job file. Important: If you notice that your SLURM compute node has the state of drained, you will need to undrain it. You can undrain the node by launching the scontrol interpreter and issuing the following commands: text ~# scontrol update NodeName=compute-0 State=DOWN Reason=\"undraining\" update NodeName=compute-0 State=RESUME","title":"Writing a job"},{"location":"together/#submitting-a-job","text":"You can use the following command to submit your job file to your micro-HPC cluster: $ sbatch research.submit","title":"Submitting a job"},{"location":"together/#evaluating-the-results","text":"After a few seconds, your job's results should be returned by SLURM. You can use the following command to browse through the results of your job: $ less research.65539.out Note: Your job ID number may defer from the one above. You should see something like the following in your less window: 1ST RESULT : +445.62 2ND RESULT : -123.45 3RD RESULT : XYZ 4TH RESULT : M565$ SUMMARY: ATOMS CHEMICALS OH MY ______ < Done > ------ \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Congratulations! You successfully ran your first job. This is the end of the workshop, so the next page will discuss where you can go from here!","title":"Evaluating the results"}]}